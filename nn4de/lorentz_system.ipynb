{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorentz System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, c.f. [wikipedia](https://en.wikipedia.org/wiki/Lorenz_system), [mathworld](http://mathworld.wolfram.com/LorenzAttractor.html), and [a solution by _Mathematica_](http://reference.wolfram.com/language/example/VisualizeTheLorenzAttractor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some $T > 0$ given, for $\\forall t \\in [0, T]$,\n",
    "\\begin{align}\n",
    "  \\frac{\\mathrm{d} x}{\\mathrm{d} t} &= \\sigma ( y - x ), \\\\\n",
    "  \\frac{\\mathrm{d} y}{\\mathrm{d} t} &= x ( \\rho - z ) - y, \\\\\n",
    "  \\frac{\\mathrm{d} z}{\\mathrm{d} t} &= x y - \\beta z,\n",
    "\\end{align}\n",
    "with boundary conditions $x(0) = x_0$, $y(0) = y_0$, and $z(0) = z_0$, for some $(x_0, y_0, z_0)$ given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "# Global parameters\n",
    "_SEED = 42\n",
    "_EPSILON = 1e-8\n",
    "\n",
    "# For reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(_SEED)\n",
    "rn.seed(_SEED)\n",
    "tf.set_random_seed(_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Helpers\n",
    "\n",
    "def ln_norm(x, n, name=None):\n",
    "  \"\"\"L`n`-norm of tensor `x`.\"\"\"\n",
    "  with tf.name_scope(name, 'L{}_Norm'.format(n), [x]):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    norm = tf.reduce_mean(x**n)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_neural_network(x, hidden_layers, output_shape, name=None):\n",
    "  \"\"\"Implement the generic neural network with the dense linear output\n",
    "  layer.\n",
    "  Args:\n",
    "    x: Tensor-like, as the input of the neural network. It's shape is\n",
    "        of `[batch_size] + x_shape`.\n",
    "    hidden_layers: List of objects of the classes in `tf.layers`.\n",
    "    output_shape: List of integers.\n",
    "  Returns:\n",
    "    The output tensor of the neural network.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'NeuralNetwork', [x]):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    \n",
    "    # Hidden layers\n",
    "    hidden = x  # initialize.\n",
    "    for layer in hidden_layers:\n",
    "      hidden = layer(hidden)\n",
    "    \n",
    "    # Output layer\n",
    "    flatten_hidden = tf.layers.flatten(hidden)\n",
    "    output_size = sum(flatten(output_shape))\n",
    "    output = tf.layers.dense(flatten_hidden, output_size)\n",
    "    output = tf.reshape(output, [-1]+output_shape)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def flatten(nested_list):\n",
    "    \"\"\"Helper. (Recursively) flatten an arbitrarily nested list.\"\"\"\n",
    "    if nested_list == []:\n",
    "        return nested_list\n",
    "    if isinstance(nested_list[0], list):\n",
    "        return flatten(nested_list[0]) + flatten(nested_list[1:])\n",
    "    return nested_list[:1] + flatten(nested_list[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary is \"re-parameterized\" by $L^n \\left( \\zeta(0) - \\zeta_0 \\right)$ for $\\zeta \\in (x, y, z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_boundary_loss(boundary, boundary_target, n, lambda_, name=None):\n",
    "  \"\"\"Make the boundary term in loss, vanishing of which makes the fitting\n",
    "  of boundary.\n",
    "  \n",
    "  $ \\mathcal{L}_{\\textrm{boundary}} := \\lambda * L^n (\\zeta(0) - \\zeta_0)$\n",
    "  for $\\zeta \\in (x, y, z)$.\n",
    "  \n",
    "  Args:\n",
    "    XXX\n",
    "    n: Positive integer, employ L-`n` norm.\n",
    "    lambda: Positive float.\n",
    "  Returns:\n",
    "    Scalar.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'BoundaryLoss', [boundary, boundary_target]):\n",
    "    boundary = tf.convert_to_tensor(boundary)\n",
    "    boundary_target = tf.convert_to_tensor(boundary_target)\n",
    "    # `delta` shall be vanishing\n",
    "    delta = boundary - boundary_target\n",
    "    boundary_loss = lambda_ * ln_norm(delta, n)\n",
    "    return boundary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ODEParameter = namedtuple('ODEParameter', 'sigma, rho, beta')\n",
    "\n",
    "def get_ode(ode_param):\n",
    "  def ode(inputs, outputs, name=None):\n",
    "    with tf.name_scope(name, 'ODE', [inputs, outputs]):\n",
    "      inputs = tf.convert_to_tensor(inputs)\n",
    "      outputs = tf.convert_to_tensor(outputs)\n",
    "      x, y, z = tf.unstack(outputs, axis=1)\n",
    "      f_value = [\n",
    "          ode_param.sigma * (y - x),\n",
    "          x * (ode_param.rho  - z) - y,\n",
    "          x * y - ode_param.beta * z,\n",
    "      ]\n",
    "      f_value = tf.stack(f_value, axis=1)\n",
    "      return f_value\n",
    "  return ode\n",
    "\n",
    "\n",
    "def make_ode_loss(inputs, outputs, grads, ode, n, name=None):\n",
    "  \"\"\"Make the ODE term in loss, vanishing if the ODE is satisfied.\n",
    "  Args:\n",
    "    inputs:\n",
    "    outputs:\n",
    "    grads:\n",
    "    ode:\n",
    "    ode_param: An instance of `ODEParameter`.\n",
    "    n:\n",
    "  Returns:\n",
    "    Scalar.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'ODELoss', [inputs, outputs, grads]):\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    outputs = tf.convert_to_tensor(outputs)\n",
    "    grads = tf.convert_to_tensor(grads)\n",
    "    deltas = grads - ode(inputs, outputs)\n",
    "    ode_loss = ln_norm(deltas, n)\n",
    "    return ode_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultOps = namedtuple('ResultOps', 'inputs, outputs, output_bulk, '\n",
    "    'output_boundary, grads, ode_loss, boundary_loss, loss')\n",
    "\n",
    "def make_loss(bulk, boundary, boundary_target, make_solution,\n",
    "              ode_param, n, lambda_, name=None):\n",
    "  \"\"\"Implements the loss.\n",
    "\n",
    "  Args:\n",
    "    bulk: Tensor-like, with the shape `[batch_size, param_space_dim]`.\n",
    "    boundary: Tensor-like, with the shape `[n_boundaries, param_space_dim]`.\n",
    "    boundary_target: Tensor-like, with the shape `[n_boundaries, phase_space_dim]`.\n",
    "    make_solution: Callable that maps input of the shape `[None, param_space_dim]`\n",
    "        to the neural network output of the shape `[None, phase_space_dim]`.\n",
    "    ode_param: An instance of `ODEParameter`.\n",
    "  Returns:\n",
    "    Scalar.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'Loss', [bulk, boundary, boundary_target]):\n",
    "    bulk = tf.convert_to_tensor(bulk)\n",
    "    boundary = tf.convert_to_tensor(boundary)\n",
    "    boundary_target = tf.convert_to_tensor(boundary_target)\n",
    "    \n",
    "    inputs = tf.concat([bulk, boundary], axis=0)\n",
    "    outputs = make_solution(inputs)\n",
    "    grads = tf.gradients(outputs, [inputs])[0]\n",
    "\n",
    "    batch_size = bulk.get_shape().as_list()[0]\n",
    "    n_boundaries = boundary.get_shape().as_list()[0]\n",
    "    output_bulk, output_boundary = tf.split(\n",
    "        outputs, [batch_size, n_boundaries], axis=0)\n",
    "\n",
    "    ode = get_ode(ode_param)\n",
    "    ode_loss = make_ode_loss(inputs, outputs, grads, ode, n)\n",
    "    boundary_loss = make_boundary_loss(boundary, boundary_target,\n",
    "                                       n, lambda_)\n",
    "    loss = ode_loss + boundary_loss + (ln_norm(output_bulk, 2) + 1e-8)**(-1)\n",
    "\n",
    "    return ResultOps(inputs, outputs, output_bulk, output_boundary,\n",
    "                     grads, ode_loss, boundary_loss, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(T, ode_param, boundary_target, n=2, lambda_=1.0, n_samples=10**3,\n",
    "         hidden_layers=[lambda x: tf.layers.dense(x, 10)]*3,\n",
    "         optimizer=None, n_iters=10**4, skip_step=10):\n",
    "\n",
    "    # -- Build graph\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    param_space_dim = 1\n",
    "    phase_space_dim = 3\n",
    "    bulk = tf.placeholder(shape=[n_samples, param_space_dim],\n",
    "                          dtype='float32', name='bulk')\n",
    "    boundary = np.zeros([1, 1], dtype='float32')\n",
    "    boundary_target = np.array(boundary_target, dtype='float32')\n",
    "         \n",
    "    def make_solution(x):\n",
    "        return make_neural_network(x, hidden_layers, [phase_space_dim])\n",
    "\n",
    "    ops = make_loss(bulk, boundary, boundary_target, make_solution,\n",
    "                    ode_param, n, lambda_)\n",
    "\n",
    "    # -- Optimization\n",
    "    \n",
    "    optimizer = optimizer if optimizer else tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(ops.loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize all `tf.Variable`s, explicit or implicit\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # -- Optimizing\n",
    "        \n",
    "        def get_bulk_value():\n",
    "            \"\"\"For feed-dict.\"\"\"\n",
    "            bulk_value = np.random.uniform(0, T,\n",
    "                size=[n_samples, param_space_dim])\n",
    "            bulk_value = bulk_value.astype('float32')\n",
    "            bulk_value = np.sort(bulk_value, axis=0)\n",
    "            return bulk_value\n",
    "        \n",
    "        # For visualization\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        # ax = fig.gca(projection='3d')  # if 3-D.\n",
    "        plt.ion()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # Iterations\n",
    "        for i in range(n_iters):\n",
    "            step = i + 1\n",
    "            iter_ops = [train_op, ops.loss, ops.ode_loss, ops.boundary_loss,\n",
    "                        ops.output_bulk]\n",
    "            bulk_value = get_bulk_value()\n",
    "            result = sess.run(iter_ops, feed_dict={bulk: bulk_value})\n",
    "            _, loss, ode_loss, boundary_loss, output_bulk = result\n",
    "\n",
    "            # Visualization\n",
    "            if step % skip_step == 0:\n",
    "                ax.clear()\n",
    "                ax.plot(output_bulk[:,0], output_bulk[:,1])\n",
    "                ax.set_title('{0} | {1:.3f} {2:.3f} {3:.3f}'\\\n",
    "                             .format(step, loss, ode_loss, boundary_loss))\n",
    "                fig.canvas.draw()\n",
    "\n",
    "            # For debugging\n",
    "            if np.isnan(loss):\n",
    "                print(step, loss, action, boundary,\n",
    "                      grad_y_bulk, x_bulk_value, y_bulk)\n",
    "                plt.plot(x_bulk_value, y_bulk)\n",
    "                plt.show()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test(T=40, ode_param=ODEParameter(10, 28, 8/3), boundary_target=[[1, 1, 1]],\n",
    "     n_iters=10**4, hidden_layers=[lambda x: tf.layers.dense(x, 100)]*5,\n",
    "     optimizer=tf.train.AdamOptimizer(1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "  * Compare with analytic result.\n",
    "  * Coherent the numerical stability at boundary and the penalty to the boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
