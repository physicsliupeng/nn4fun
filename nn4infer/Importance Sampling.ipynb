{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for importance sampling supplemented variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As the visualization of shallow neural network on MNIST dataset shows, the fitting of the PDF of the posterior via variational inference needs further finization. This calls for adding more (trainable) degree of freedom to the inferencer. At the same time, however, we shall keep both the PDF and the sampler of the inference-distribution explicit. Thus, the approach of adding these degree of freedom onto the parameterization of the inference-distribution would be too hard to establish. Instead, we decide to add them onto the weights of the samples sampled from the inference-distribution, thus treat this approach as an importance-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Furthermore, the weight is a map from the parameter-space to $\\mathbb{R}$, which thus avoids the redundancy of degree of freedom as the approach of neural network sampler (which maps from some low-dimensional hidden space to the high-dimensional parameter-space).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loss-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let $\\theta \\in \\mathbb{R}^{N_d}$ the parameters, and $p(\\theta)$ the PDF of the posterior over the parameter-space. Let $q(\\theta; z)$ the inference distribution, with parameter $z$ to be trained. Let\n",
    "\\begin{equation}\n",
    "  \\hat{q}(\\theta; z, w) := \\omega (\\theta; w) q(\\theta, z),\n",
    "\\end{equation}\n",
    "where $\\omega (\\theta; w)$, mapping to $\\mathbb{R}$, is the weights of the importance sampling, with parameter $w$ to be trained.\n",
    "\n",
    "The loss-function is\n",
    "\\begin{align}\n",
    "  \\mathcal{L} (z, w)\n",
    "    & := \\textrm{KL}(\\hat{q} \\| p) \\\\\n",
    "    & = \\int \\hat{q} (\\theta)\n",
    "          \\left[ \\ln \\hat{q} (\\theta; z, w) - \\ln p (\\theta) \\right]\n",
    "        d \\theta \\\\\n",
    "    & = \\int q(\\theta) \\omega (\\theta; w)\n",
    "          \\left[ \\ln \\omega (\\theta; w) + \\ln q (\\theta; z)\n",
    "                 - \\ln p (\\theta) \\right]\n",
    "        d \\theta \\\\\n",
    "    & = \\mathbb{E}_{\\theta_s \\sim q(z)}\n",
    "          \\left\\{ \\omega (\\theta_s; w)\n",
    "            \\left[ \\ln \\omega (\\theta_s; w) + \\ln q (\\theta_s; z)\n",
    "                   - \\ln p (\\theta_s) \\right] \\right\\},\n",
    "\\end{align}\n",
    "where $s = 1, \\ldots, N_s$. Further, for $\\theta \\in \\{\\theta_1, \\ldots, \\theta_{N_s}\\} \\sim q(z)$, let\n",
    "\\begin{equation}\n",
    "  \\omega(\\theta; w) := \\textrm{softmax} \\left( \\zeta (\\theta; w) \\right) * N_s,\n",
    "\\end{equation}\n",
    "the normalization of $\\hat{q}(\\theta; z, w)$ can always be ensured:\n",
    "\\begin{align}\n",
    "  1 & = \\int \\hat{q}(\\theta; z, w) d\\theta \\\\\n",
    "    & = \\mathbb{E}_{\\theta_s \\sim q(z)} \\left[ \\omega (\\theta; w) \\right] \\\\\n",
    "    & = N_s * \\mathbb{E}_{\\theta_s \\sim q(z)}\n",
    "                \\left[ \\textrm{softmax} \\left( \\zeta (\\theta; w) \\right) \\right] \\\\\n",
    "    & = N_s * \\frac{1}{N_s}\n",
    "        \\sum_s \\textrm{softmax}_s \\left( \\zeta (\\theta; w) \\right) \\\\\n",
    "    & = N_s \\frac{1}{N_s} \\times 1.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This methods extends the representability of pure variational inference by adding a weight-function. This weight-function can be implemented by a neural network by the utility of its universality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We do not prefer a large variance of the weights, thus a penalty on this variance in the loss is called for. Thus\n",
    "\\begin{align}\n",
    "  \\mathcal{L}(z, w)\n",
    "    & \\to \\mathcal{L}(z, w; \\sigma) \\\\\n",
    "    & = \\mathbb{E}_{\\theta_s \\sim q(z)}\n",
    "          \\left\\{ \\omega (\\theta_s; w)\n",
    "            \\left[ \\ln \\omega (\\theta_s; w) + \\ln q (\\theta_s; z)\n",
    "                   - \\ln p (\\theta_s) \\right] \\right\\} \\\\\n",
    "     & + \\mathbb{E}_{\\theta_s \\sim q(z)}\n",
    "           \\left\\{ \\left(\n",
    "             \\frac{ \\omega(\\theta_s; w) - 1 }{ \\sigma }\n",
    "             \\right)^2 \\right\\},\n",
    "\\end{align}\n",
    "where $\\sigma$ is a hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The machine will automatically minimizes this loss by tuning the trainable parameters $(z, w)$, and then find a \"harmonic\" way of combining the $\\omega$ and $q$, so as to represent the target $p(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Measurement of Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The performance of the inference can be measured explicitly. For each sample $\\theta$, define error:\n",
    "\\begin{equation}\n",
    "  \\mathcal{E} (\\theta; z, w)\n",
    "    := \\ln p(\\theta) - \\ln q(\\theta, z) - \\ln \\omega(\\theta, w).\n",
    "\\end{equation}\n",
    "If the inference works, we would expect that the $\\{\\mathcal{E}(\\theta_s) \\mid s = 1, \\ldots, N_s\\}$ distributes around the zero-point.\n",
    "\n",
    "This measurement of performance can also help us in fine-tuning the hyper-parameters, like the $\\sigma$, or those in the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuiruge/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as tfd\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x, limit=30):\n",
    "  if x > limit:\n",
    "    return x\n",
    "  else:\n",
    "    return np.log(1.0 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LossInfo = namedtuple('LossInfo',\n",
    "    ('samples, sample_logits, weights, log_weights, log_ps,'\n",
    "      'log_qs, kl_divergence, weight_penalty, loss, errors'))\n",
    "\n",
    "def make_loss(log_p, inference_dist, logits,\n",
    "              logits_scale=1e+2, n_samples=10,\n",
    "              use_logits=True):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    log_p: Callable from tensor of the shape `[n_dims]` to scalar.\n",
    "    inference_dist: An instance of `tfd.Distribution`.\n",
    "    logits: Callable from tensor of the shape `[n_samples, n_dims]`\n",
    "      to tensor of the shape `[n_samples]`.\n",
    "    logits_scale: Positive float.\n",
    "    n_samples: Positive integer.\n",
    "  Returns:\n",
    "    An instance of `LossInfo`.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('Samples'):\n",
    "    # shape `[n_samples, n_dims]`\n",
    "    samples = inference_dist.sample(n_samples)\n",
    "  with tf.name_scope('Logits'):\n",
    "    # shape: `[n_samples]`\n",
    "    sample_logits = logits(samples)\n",
    "  with tf.name_scope('KLDivergence'):\n",
    "    # shape: `[n_samples]`\n",
    "    weights = tf.nn.softmax(sample_logits) * n_samples\n",
    "    # shape: `[n_samples]`\n",
    "    log_weights = tf.log(weights)\n",
    "    # The batch-supplement may not ensured in `log_p`,\n",
    "    # so we employ `tf.map_fn` for vectorization\n",
    "    # shape: `[n_samples]`\n",
    "    log_ps = tf.map_fn(log_p, samples)\n",
    "    # Notice `tfd.Distribution.log_prob()` is batch-supplemented,\n",
    "    # shape: `[n_samples]`\n",
    "    log_qs = inference_dist.log_prob(samples)\n",
    "    # shape: `[]`\n",
    "    kl_divergence = tf.reduce_mean(\n",
    "      weights * (log_weights + log_qs - log_ps),\n",
    "      axis=0)\n",
    "  with tf.name_scope('Penalty'):\n",
    "    # shape: `[n_samples]`\n",
    "    delta_weights = (weights - 1.0) / logits_scale\n",
    "    # shape: `[]`\n",
    "    penalty = tf.reduce_mean(tf.square(delta_weights), axis=0)\n",
    "  with tf.name_scope('Loss'):\n",
    "    # shape: `[]`\n",
    "    # loss = kl_divergence + weight_penalty\n",
    "    loss = kl_divergence * (1.0 + weight_penalty)\n",
    "  with tf.name_scope('Errors'):\n",
    "    # shape: `[n_samples]`\n",
    "    errors = log_ps - log_qs - log_weights\n",
    "  return LossInfo(samples, sample_logits, weights, log_weights, log_ps,\n",
    "                  log_qs, kl_divergence, weight_penalty, loss, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### High-dimensional Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Set up graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The target is the PDF of distribution\n",
    "$\\mathcal{N} \\left(0, \\textrm{softplus}(10)\\right)$\n",
    "on $\\mathbb{R}^{N_d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_dims = 10000\n",
    "target_dist = tfd.Independent(\n",
    "    tfd.NormalWithSoftplusScale(loc=tf.zeros(n_dims),\n",
    "                                scale=10*tf.ones(n_dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('InferenceDistribution'):\n",
    "  loc = tf.get_variable('loc', [n_dims], 'float32')\n",
    "  scale = tf.get_variable('scale', [n_dims], 'float32')\n",
    "  inference_dist = tfd.Independent(\n",
    "      tfd.NormalWithSoftplusScale(loc, scale))\n",
    "\n",
    "def logits(samples, n_hiddens=5):\n",
    "  \"\"\"The `logits` argument of `make_loss()`.\"\"\"\n",
    "  with tf.name_scope('LogitsNeuralNetwork'):\n",
    "    # shape: `[n_samples, n_hiddens]`\n",
    "    hiddens = tf.layers.dense(samples, n_hiddens,\n",
    "                              activation=tf.nn.leaky_relu)\n",
    "    # shape: `[n_samples, 1]`\n",
    "    outputs = tf.layers.dense(hiddens, 1)\n",
    "    # shape: `[n_samples]`\n",
    "    return tf.squeeze(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_info = make_loss(target_dist.log_prob, inference_dist, logits,\n",
    "                      logits_scale=10.0)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(loss_info.loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Loss (before):', sess.run(loss_info.loss))\n",
    "\n",
    "loss_vals = []\n",
    "n_iters = 20000\n",
    "for step in range(1, n_iters+1):\n",
    "  sess.run(train_op)\n",
    "  \n",
    "  # The value of loss can sometimes temporally be `NaN`, and in\n",
    "  # the next `sess.run()` becomes non-`NaN` (strange!). So, we\n",
    "  # employ the following strategy:\n",
    "  loss_val = sess.run(loss_info.loss)\n",
    "  n_trials = 0\n",
    "  while np.isnan(loss_val) and n_trials < 10:\n",
    "    loss_val = sess.run(loss_info.loss)\n",
    "    n_trials += 1\n",
    "    if n_trials == 9:\n",
    "      print(sess.run([loss_info.log_weights, loss_info.log_ps,\n",
    "                      loss_info.log_qs, loss_info.kl_divergence,\n",
    "                      loss_info.weight_penalty]))\n",
    "  if n_trials == 9:\n",
    "    print('Always `NaN`, finally stopped at step {}.'.format(step))\n",
    "    break  \n",
    "  loss_vals.append(loss_val)\n",
    "  \n",
    "print('Loss (after):', sess.run(loss_info.loss))\n",
    "print('Weight penalty (after):', sess.run(loss_info.weight_penalty))\n",
    "\n",
    "# Visualize\n",
    "plt.plot(loss_vals, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize the trained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "log_weights = []\n",
    "errors = []\n",
    "for i in range(100):\n",
    "  result = sess.run([loss_info.samples, loss_info.log_weights,\n",
    "                     loss_info.errors])\n",
    "  sample_vals, log_weight_vals, error_vals = result\n",
    "  samples += [_ for _ in sample_vals]\n",
    "  log_weights += [_ for _ in log_weight_vals]\n",
    "  errors += [_ for _ in error_vals]\n",
    "samples = np.array(samples)\n",
    "log_weights = np.array(log_weights)\n",
    "errors = np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist(samples[:,0]/softplus(10), bins=50, density=True,\n",
    "         label='samples/softplus(10)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(log_weights, bins=50, density=True,\n",
    "         label='log-weights')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(errors, bins=50, density=True,\n",
    "         label='errors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Conclusion: so far so good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
