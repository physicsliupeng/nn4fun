{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Variation Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brachistochrone Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, c.f. [here](https://baike.baidu.com/item/%E6%9C%80%E9%80%9F%E9%99%8D%E7%BA%BF%E9%97%AE%E9%A2%98)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a trajectory $y(x)$ for $\\forall x \\in [0, 1]$, the action (loss), as the total time spent, is given by\n",
    "$$ L[y] = \\int_0^1 d x \\frac{ \\sqrt{ 1 + {y^{\\prime}}^2 (x) } } { \\sqrt{- 2 y(x)} } , $$\n",
    "with boundary condition\n",
    "$$ y(0) = 0 \\;, y(1) = -1 , $$\n",
    "where we have set gravity-constant $g$ as unit. Ensure that $y(x) \\le 0, \\forall x \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tdb\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employ Monte-Carlo approximation. Let $\\{ x_i: i = 1, \\ldots, N \\} \\sim \\text{Uniform}(0, 1)$, the action with boundary condition and penalty becomes\n",
    "\\begin{equation}\n",
    "  L[y] = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{ \\sqrt{ 1 + {y^{\\prime}}^2 (x_i) } } { \\sqrt{-2 y(x_i)} }\n",
    "       + \\frac{\\lambda_1}{2} \\sum_{ x_b \\in \\{0,1\\} } d(y(x_b), 0)\n",
    "       + \\frac{\\lambda_2}{N} \\sum_{i=1}^{N} \\text{sign}( y_i > 0 )\n",
    "\\end{equation}\n",
    "with some adjustable hyper-parameter $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^+$ and some pre-defined distance $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Helpers\n",
    "\n",
    "def l1_norm(x):\n",
    "    \"\"\"L1-norm.\"\"\"\n",
    "    return np.mean(np.abs(x))\n",
    "\n",
    "def flatten(nested_list):\n",
    "    \"\"\"Helper. (Recursively) flatten an arbitrarily nested list.\"\"\"\n",
    "    if nested_list == []:\n",
    "        return nested_list\n",
    "    if isinstance(nested_list[0], list):\n",
    "        return flatten(nested_list[0]) + flatten(nested_list[1:])\n",
    "    return nested_list[:1] + flatten(nested_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_neural_network(x, hidden_layers, output_shape, name=None):\n",
    "  \"\"\"Implement the neural network with dense hidden layers, each with\n",
    "  the same activation-function `activation`, and regularized by a\n",
    "  dropout layer with dropout-rate as the default (i.e. 0.5).\n",
    "  Args:\n",
    "    x: Tensor-like, as the input of the neural network. It's shape is\n",
    "        of `[batch_size] + x_shape`.\n",
    "    hidden_layers: List of objects of the classes in `tf.layers`.\n",
    "    output_shape: List of integers.\n",
    "  Returns:\n",
    "    The output tensor of the neural network.\n",
    "  \"\"\"\n",
    "  x = tf.convert_to_tensor(x, name='x')\n",
    "\n",
    "  with tf.name_scope(name, 'NeuralNetwork', [x]):\n",
    "    \n",
    "    # Hidden layers\n",
    "    hidden = x  # initialize.\n",
    "    for layer in hidden_layers:\n",
    "      hidden = layer(hidden)\n",
    "    \n",
    "    # Output layer\n",
    "    flatten_hidden = tf.layers.flatten(hidden)\n",
    "    output_size = sum(flatten(output_shape))\n",
    "    output = tf.layers.dense(flatten_hidden, output_size)\n",
    "    output = tf.reshape(output, [-1]+output_shape)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action(y, grad_y, epsilon=1e-2, name=None):\n",
    "    \"\"\"Implements the action.\n",
    "    Args:\n",
    "      y: Tensor-like, with shape `[batch_size] + y_shape`.\n",
    "      grad_y: Tensor-like, with shape `[batch_size] + y_shape`.\n",
    "    Returns:\n",
    "      Scalar.\n",
    "    \"\"\"\n",
    "    y = tf.convert_to_tensor(y, name='y')\n",
    "    grad_y = tf.convert_to_tensor(grad_y, name='grad_y')\n",
    "    with tf.name_scope(name, 'Action', [y, grad_y]):\n",
    "      delta_height = tf.where(y < 0.0, y, tf.zeros(y.shape))\n",
    "      lagrangians = tf.truediv(\n",
    "          tf.sqrt(1.0 + tf.square(grad_y)),\n",
    "          tf.sqrt(- 2.0 * delta_height + epsilon))\n",
    "      return tf.reduce_mean(lagrangians)\n",
    "    \n",
    "    \n",
    "def make_distance(x, y, name=None):\n",
    "  \"\"\"Mean square distance between tensors `x` and `y`.\"\"\"\n",
    "  with tf.name_scope(name, 'MeanSquareDistance', [x, y]):\n",
    "    distance = tf.reduce_mean(tf.square(x - y))\n",
    "  return distance\n",
    "\n",
    "\n",
    "LossOps = namedtuple('LossOps',\n",
    "    'loss, y, grad_y, y_within, y_boundary, action_part, boundary_part')\n",
    "\n",
    "def make_loss(x_within, x_boundary, make_neural_network,\n",
    "              lambda_1=50.0, lambda_2=50.0, name=None):\n",
    "  \"\"\"Implements the loss.\n",
    "  Args:\n",
    "    x_within: Tensor-like, with shape `[batch_size] + x_shape`, as the\n",
    "        non-boundary values of the input to the neural network.\n",
    "    x_boundary: Tensor-like, with shape `[n_boundaries] + x_shape`, as\n",
    "        the boundary values of the input to the neural network.\n",
    "    make_neural_network: Callable that maps `x` to the neural network\n",
    "        output.\n",
    "  Returns:\n",
    "    Scalar, as the loss.\n",
    "  \"\"\"\n",
    "  x_within = tf.convert_to_tensor(x_within, name='x_within')\n",
    "  x_boundary = tf.convert_to_tensor(x_boundary, name='x_boundary')\n",
    "    \n",
    "  x = tf.concat([x_within, x_boundary], axis=0)    \n",
    "  y = make_neural_network(x)    \n",
    "\n",
    "  batch_size = x_within.get_shape().as_list()[0]\n",
    "  n_boundary = x_boundary.get_shape().as_list()[0]\n",
    "  y_within, y_boundary = tf.split(y, [batch_size, n_boundary],\n",
    "                                  axis=0)\n",
    "  grad_y = tf.gradients(y, [x])[0]\n",
    "\n",
    "  with tf.name_scope(name, 'Loss'):\n",
    "    lambda_1 = tf.constant(lambda_1, dtype='float32')\n",
    "    action_part = action(y, grad_y)\n",
    "    boundary = tf.constant([[0.0], [-1.0]], dtype='float32')\n",
    "    boundary_part = lambda_1 * make_distance(y_boundary, boundary)\n",
    "    #loss = action_part + boundary_part\n",
    "    loss = action_part * (1.0 + boundary_part)  # for arbitrary choice.\n",
    "\n",
    "  return LossOps(loss, y, grad_y, y_within, y_boundary,\n",
    "                 action_part, boundary_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3.52954 1.12631 2.13372 0.586925 0.578641\n",
      "199 1.74438 1.54151 0.131606 0.906131 0.587868\n",
      "299 1.60627 1.51159 0.062636 0.816607 0.653749\n",
      "399 1.97644 1.86824 0.057915 1.02035 0.458649\n",
      "499 1.83789 1.71825 0.0696313 0.928167 0.547888\n",
      "599 1.71985 1.57591 0.0913435 0.973092 0.574221\n",
      "699 1.86996 1.71021 0.0934076 0.928708 0.545971\n",
      "799 1.54347 1.40035 0.102199 0.812742 0.681497\n",
      "899 1.72864 1.55384 0.112493 0.945881 0.594277\n",
      "999 1.69918 1.49292 0.138164 0.877558 0.624862\n",
      "1099 1.52795 1.40569 0.0869782 0.908373 0.650719\n",
      "1199 1.65774 1.50479 0.101643 0.98389 0.604766\n",
      "1299 1.5702 1.38083 0.137145 0.883297 0.67076\n",
      "1399 1.81255 1.63917 0.105776 0.869581 0.576072\n",
      "1499 1.56748 1.44347 0.0859151 0.870972 0.663519\n",
      "1599 2.00605 1.89762 0.0571396 1.0949 0.45868\n",
      "1699 1.51555 1.39218 0.0886187 0.897487 0.666028\n",
      "1799 1.61903 1.47265 0.099397 0.936814 0.638071\n",
      "1899 1.99048 1.812 0.098498 0.933404 0.525009\n",
      "1999 1.70676 1.5808 0.0796823 0.974652 0.620397\n",
      "2099 1.6799 1.52665 0.100387 0.941374 0.629618\n",
      "2199 1.82813 1.69795 0.0766712 0.849229 0.599953\n",
      "2299 1.71262 1.59613 0.0729831 0.881961 0.609597\n",
      "2399 2.07136 1.95443 0.0598282 0.97854 0.489553\n",
      "2499 1.91681 1.78005 0.0768283 0.935592 0.544518\n",
      "2599 1.5446 1.45681 0.060261 0.815163 0.679135\n",
      "2699 1.5061 1.28416 0.172828 0.810609 0.734762\n",
      "2799 1.58617 1.38414 0.145965 0.819903 0.711767\n",
      "2899 1.72711 1.57495 0.096615 0.916535 0.644059\n",
      "2999 1.67442 1.53536 0.0905682 0.993175 0.632674\n",
      "3099 2.03418 1.9017 0.0696671 0.976143 0.528806\n",
      "3199 1.83176 1.70633 0.0735061 0.964431 0.59152\n",
      "3299 1.59735 1.4878 0.0736339 0.82924 0.661875\n",
      "3399 1.66055 1.55786 0.065922 0.819523 0.65245\n",
      "3499 1.53239 1.42308 0.0768081 0.804262 0.677517\n",
      "3599 1.89059 1.70664 0.107783 1.07679 0.59144\n",
      "3699 1.64297 1.54448 0.0637688 0.832911 0.653337\n",
      "3799 1.75459 1.64513 0.0665376 0.992651 0.610746\n",
      "3899 1.40779 1.30228 0.0810167 0.670314 0.741698\n",
      "3999 1.9122 1.79968 0.0625219 1.04636 0.546405\n",
      "4099 1.37156 1.26576 0.0835831 0.800214 0.723886\n",
      "4199 1.68093 1.54615 0.0871702 0.789065 0.637311\n",
      "4299 1.74226 1.60495 0.0855553 0.915415 0.613048\n",
      "4399 1.80589 1.73231 0.0424747 1.03801 0.58913\n",
      "4499 1.61182 1.47851 0.0901606 0.903899 0.65261\n",
      "4599 1.40697 1.29405 0.0872604 0.736455 0.716448\n",
      "4699 2.13534 1.94824 0.096036 1.10683 0.568561\n",
      "4799 1.59916 1.46103 0.0945419 0.775377 0.63526\n",
      "4899 1.33078 1.24938 0.0651486 0.678684 0.723987\n",
      "4999 1.75238 1.57684 0.111323 0.987509 0.633352\n",
      "5099 2.02575 1.92073 0.0546795 0.944218 0.555799\n",
      "5199 1.70307 1.56664 0.0870831 0.937455 0.63649\n",
      "5299 1.36516 1.26859 0.0761229 0.784129 0.719583\n",
      "5399 1.31995 1.22861 0.0743463 0.679356 0.771698\n",
      "5499 1.83827 1.7253 0.0654805 0.863186 0.650821\n",
      "5599 1.93692 1.78038 0.0879248 1.02986 0.680887\n",
      "5699 1.89421 1.69507 0.117482 0.987207 0.597513\n",
      "5799 1.35475 1.27061 0.0662173 0.809743 0.70758\n",
      "5899 1.68979 1.56455 0.0800493 0.873917 0.649664\n",
      "5999 1.4891 1.39631 0.0664494 0.812387 0.694165\n",
      "6099 1.69335 1.58088 0.0711452 0.896682 0.672215\n",
      "6199 2.12311 1.97405 0.0755059 1.11201 0.614748\n",
      "6299 1.66962 1.57475 0.0602433 0.936838 0.643864\n",
      "6399 1.47967 1.36168 0.0866518 0.773087 0.699578\n",
      "6499 2.00508 1.84189 0.0886015 0.930019 0.636666\n",
      "6599 2.24119 2.08634 0.0742184 1.14626 0.552899\n",
      "6699 1.30546 1.19014 0.0968966 0.72613 0.756539\n",
      "6799 1.46398 1.38365 0.0580506 0.842796 0.716826\n",
      "6899 1.69494 1.51008 0.122416 0.801918 0.674149\n",
      "6999 1.58152 1.48125 0.0676912 0.869757 0.677975\n",
      "7099 1.6264 1.49135 0.0905553 0.865899 0.628432\n",
      "7199 1.64238 1.54593 0.0623896 0.799045 0.671153\n",
      "7299 1.53514 1.42667 0.0760288 0.826546 0.682374\n",
      "7399 1.91 1.79846 0.0620223 1.03554 0.624276\n",
      "7499 1.55065 1.43597 0.0798672 0.962862 0.642453\n",
      "7599 1.93282 1.78192 0.084689 0.970648 0.61454\n",
      "7699 1.4712 1.34185 0.0963949 0.743188 0.750542\n",
      "7799 1.2825 1.23222 0.0408067 0.788477 0.709619\n",
      "7899 1.67403 1.57553 0.0625155 0.943463 0.613087\n",
      "7999 1.87084 1.80022 0.0392304 1.00806 0.660169\n",
      "8099 1.73193 1.65769 0.0447876 1.05777 0.57726\n",
      "8199 1.51547 1.41201 0.073272 0.824846 0.685028\n",
      "8299 1.65395 1.56878 0.0542934 0.985938 0.599443\n",
      "8399 1.31081 1.20255 0.0900239 0.748167 0.720831\n",
      "8499 1.55321 1.4426 0.0766783 0.748137 0.670138\n",
      "8599 1.54525 1.42916 0.0812237 0.80086 0.674601\n",
      "8699 1.73779 1.66674 0.0426291 0.910964 0.631664\n",
      "8799 1.61115 1.49843 0.0752269 0.885473 0.647199\n",
      "8899 1.76794 1.66185 0.0638355 0.896153 0.614872\n",
      "8999 1.3449 1.20648 0.114726 0.673289 0.728094\n",
      "9099 1.7005 1.6044 0.0598975 0.804302 0.629394\n",
      "9199 1.51108 1.37322 0.100391 0.734173 0.731199\n",
      "9299 1.61468 1.49166 0.0824706 0.858193 0.690944\n",
      "9399 1.81828 1.64888 0.102736 0.894644 0.587725\n",
      "9499 1.59995 1.522 0.0512162 0.840289 0.691082\n",
      "9599 1.47528 1.37299 0.074503 0.819538 0.66796\n",
      "9699 1.67842 1.53564 0.0929732 0.900308 0.665548\n",
      "9799 1.14417 1.05681 0.0826627 0.598514 0.790447\n",
      "9899 1.57474 1.43869 0.0945598 0.874103 0.656827\n",
      "9999 1.65615 1.55558 0.0646487 0.844083 0.637595\n",
      "10099 1.62245 1.52976 0.0605905 0.830961 0.689124\n",
      "10199 1.87533 1.6848 0.113089 0.929372 0.547103\n",
      "10299 1.31803 1.24386 0.0596273 0.747444 0.74806\n",
      "10399 1.40318 1.33572 0.0505044 0.82952 0.681877\n",
      "10499 1.46669 1.38248 0.0609095 0.787259 0.68601\n",
      "10599 1.43675 1.3471 0.0665503 0.714478 0.729278\n",
      "10699 2.06202 1.94044 0.0626594 1.00273 0.622973\n",
      "10799 1.63993 1.54359 0.0624125 0.905428 0.664584\n",
      "10899 1.461 1.34619 0.0852869 0.833849 0.69527\n",
      "10999 1.22501 1.09274 0.12105 0.642696 0.753741\n",
      "11099 1.26208 1.17411 0.0749317 0.716529 0.713534\n",
      "11199 1.72698 1.61357 0.0702835 0.862895 0.641423\n",
      "11299 1.40609 1.30714 0.0757026 0.691688 0.749378\n",
      "11399 2.20487 2.01935 0.0918716 1.17891 0.591564\n",
      "11499 1.74748 1.62364 0.0762702 0.905308 0.669709\n",
      "11599 1.7969 1.65251 0.0873778 0.940446 0.599382\n",
      "11699 1.68084 1.52301 0.103632 0.866416 0.646931\n",
      "11799 1.55823 1.41849 0.098514 0.790862 0.660802\n",
      "11899 1.2496 1.1724 0.0658453 0.709571 0.722015\n",
      "11999 1.73706 1.62152 0.0712525 0.851742 0.673732\n",
      "12099 1.42504 1.35738 0.0498449 0.683937 0.728774\n",
      "12199 1.6696 1.54184 0.082863 0.92485 0.614277\n",
      "12299 1.53098 1.40855 0.0869149 0.792998 0.639207\n",
      "12399 1.35969 1.26824 0.072115 0.694207 0.775707\n",
      "12499 1.42151 1.3406 0.0603553 0.813759 0.673054\n",
      "12599 1.36886 1.26716 0.08026 0.78008 0.704215\n",
      "12699 1.66213 1.56552 0.0617126 0.955222 0.668852\n",
      "12799 1.47361 1.38802 0.0616623 0.795796 0.640692\n",
      "12899 1.50941 1.40937 0.0709834 0.792179 0.656037\n",
      "12999 1.61637 1.50299 0.0754361 0.919031 0.620386\n",
      "13099 1.53655 1.44137 0.0660337 0.826011 0.691348\n",
      "13199 1.799 1.69171 0.0634262 0.954279 0.616225\n",
      "13299 1.32221 1.20957 0.0931299 0.644868 0.72803\n",
      "13399 1.80906 1.65584 0.0925353 1.07043 0.603462\n",
      "13499 1.44507 1.36882 0.0557019 0.809935 0.719834\n",
      "13599 1.62719 1.51728 0.0724349 0.861266 0.692435\n",
      "13699 1.81504 1.7237 0.0529906 0.925554 0.694722\n",
      "13799 2.3184 2.1622 0.0722387 1.23925 0.622744\n",
      "13899 1.69633 1.53997 0.101536 0.921921 0.648856\n",
      "13999 1.7216 1.59504 0.0793429 0.934174 0.660206\n",
      "14099 1.25842 1.18882 0.0585509 0.69979 0.736916\n",
      "14199 1.47782 1.40099 0.0548414 0.870893 0.654503\n",
      "14299 1.23811 1.16683 0.0610898 0.560216 0.773799\n",
      "14399 1.53614 1.42486 0.078094 0.769424 0.71064\n",
      "14499 1.51795 1.39558 0.0876818 0.875012 0.659277\n",
      "14599 1.45041 1.33232 0.08863 0.78946 0.664124\n",
      "14699 1.62486 1.50117 0.082398 0.826548 0.629972\n",
      "14799 1.50156 1.39404 0.0771232 0.792175 0.664308\n",
      "14899 2.15692 2.0205 0.0675153 1.0415 0.585781\n",
      "14999 1.66456 1.53953 0.0812144 0.718765 0.677955\n",
      "15099 1.75399 1.6498 0.0631552 0.868437 0.661636\n",
      "15199 1.32023 1.21455 0.0870142 0.710591 0.75829\n",
      "15299 1.55924 1.45601 0.0708975 0.830415 0.72223\n",
      "15399 1.343 1.25938 0.0663901 0.657757 0.733285\n",
      "15499 2.10942 1.97497 0.0680758 1.05596 0.630404\n",
      "15599 1.96922 1.83268 0.0745018 1.01141 0.641381\n",
      "15699 1.43039 1.35504 0.0556064 0.781961 0.718187\n",
      "15799 2.03521 1.88741 0.0783091 1.0037 0.618739\n",
      "15899 1.35094 1.25367 0.0775945 0.801986 0.694705\n",
      "15999 1.59738 1.50458 0.0616815 0.894935 0.666267\n",
      "16099 2.2139 2.08046 0.0641385 1.16228 0.564054\n",
      "16199 1.32016 1.22854 0.0745816 0.755084 0.731418\n",
      "16299 1.47855 1.35568 0.0906349 0.745634 0.715733\n",
      "16399 2.00556 1.82256 0.10041 0.895606 0.542836\n",
      "16499 1.87766 1.75581 0.0693942 0.920065 0.631876\n",
      "16599 1.39743 1.2813 0.0906345 0.799433 0.689713\n",
      "16699 2.16834 2.00232 0.0829115 1.04925 0.588833\n",
      "16799 1.88324 1.74238 0.080846 0.950405 0.558477\n",
      "16899 1.56966 1.40079 0.120556 0.821536 0.670089\n",
      "16999 1.52222 1.42775 0.0661675 0.795333 0.674348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17099 1.64981 1.5242 0.0824084 0.936264 0.653134\n",
      "17199 1.77787 1.66955 0.0648838 0.924692 0.610105\n",
      "17299 1.4536 1.36154 0.0676152 0.808796 0.700837\n",
      "17399 1.59676 1.50401 0.0616667 0.863499 0.64301\n",
      "17499 1.79406 1.60747 0.116077 0.902848 0.572838\n",
      "17599 1.84748 1.74882 0.0564165 0.897024 0.639906\n",
      "17699 1.47603 1.37747 0.0715491 0.783329 0.712057\n",
      "17799 1.22649 1.14539 0.0708036 0.695657 0.75055\n",
      "17899 1.31793 1.19991 0.0983588 0.716133 0.759088\n",
      "17999 1.98129 1.84919 0.0714356 1.06702 0.634138\n",
      "18099 1.54579 1.42588 0.0840976 0.684706 0.696552\n",
      "18199 1.48908 1.39874 0.0645854 0.832966 0.689309\n",
      "18299 1.67034 1.53668 0.0869779 0.912315 0.668641\n",
      "18399 1.24041 1.12081 0.106703 0.664112 0.756847\n",
      "18499 1.96814 1.85596 0.0604452 1.09177 0.571538\n",
      "18599 1.74127 1.62206 0.0734977 0.885514 0.638308\n",
      "18699 1.94615 1.81967 0.0695068 0.951838 0.592021\n",
      "18799 1.61671 1.54695 0.0450983 0.852606 0.682323\n",
      "18899 1.67547 1.54595 0.0837788 0.860593 0.65743\n",
      "18999 1.974 1.82352 0.0825205 0.947949 0.639846\n",
      "19099 1.68506 1.53689 0.0964124 0.775842 0.659998\n",
      "19199 2.05895 1.87136 0.100245 1.09877 0.58934\n",
      "19299 1.66869 1.5607 0.0691923 0.947613 0.645889\n",
      "19399 1.7465 1.61487 0.0815157 0.917902 0.622187\n",
      "19499 1.75313 1.63815 0.0701872 0.946871 0.625679\n",
      "19599 1.7244 1.55707 0.107463 0.993007 0.606256\n",
      "19699 1.56741 1.46503 0.0698831 0.853176 0.661845\n",
      "19799 1.61428 1.49829 0.0774139 0.894217 0.649928\n",
      "19899 1.45684 1.3453 0.0829136 0.712809 0.712454\n",
      "19999 1.89778 1.78256 0.0646399 0.999619 0.647958\n",
      "20099 1.37406 1.28339 0.0706551 0.769401 0.743759\n",
      "20199 1.77733 1.66965 0.0644887 1.04571 0.610212\n",
      "20299 1.77346 1.66786 0.063316 1.02016 0.580655\n",
      "20399 1.60108 1.51138 0.0593458 0.860767 0.668351\n",
      "20499 1.71399 1.606 0.0672435 0.885322 0.641233\n",
      "20599 1.27481 1.19819 0.0639399 0.751277 0.727011\n",
      "20699 1.74061 1.59755 0.0895486 0.936934 0.613682\n",
      "20799 1.56417 1.46044 0.0710289 0.890273 0.669832\n",
      "20899 1.41354 1.33152 0.0616054 0.813528 0.717765\n",
      "20999 1.45046 1.36103 0.0657061 0.743016 0.694262\n",
      "21099 1.85271 1.68212 0.101414 0.995066 0.627895\n",
      "21199 1.42892 1.33199 0.0727733 0.750316 0.728831\n",
      "21299 1.62797 1.53245 0.0623352 0.910648 0.658175\n",
      "21399 2.05996 1.94581 0.0586613 1.04592 0.591256\n",
      "21499 1.36163 1.25869 0.0817824 0.766839 0.70174\n",
      "21599 1.77569 1.63839 0.0837971 0.872674 0.686408\n",
      "21699 1.60142 1.48119 0.0811687 0.955568 0.642793\n",
      "21799 2.08587 1.93136 0.0799985 1.0553 0.549666\n",
      "21899 1.62416 1.5159 0.0714107 0.71808 0.734476\n",
      "21999 1.29908 1.22805 0.0578437 0.769888 0.704901\n",
      "22099 2.07398 1.94435 0.0666721 0.982246 0.642498\n",
      "22199 1.57048 1.50531 0.0432926 0.885042 0.666691\n",
      "22299 1.42135 1.33051 0.0682708 0.758283 0.724661\n",
      "22399 1.38223 1.29481 0.0675183 0.803163 0.681417\n",
      "22499 1.67251 1.55465 0.0758095 1.011 0.605642\n",
      "22599 1.30905 1.20399 0.0872602 0.682508 0.735175\n",
      "22699 2.09972 1.92644 0.0899499 0.952771 0.633142\n",
      "22799 1.56862 1.48439 0.0567436 0.838169 0.625667\n",
      "22899 1.629 1.49919 0.0865857 0.825503 0.701577\n",
      "22999 1.6317 1.49457 0.0917494 0.858472 0.650591\n",
      "23099 1.85773 1.71431 0.083661 1.08912 0.600779\n",
      "23199 1.31592 1.22703 0.0724407 0.676375 0.719835\n",
      "23299 1.69998 1.56153 0.0886664 0.86272 0.708302\n",
      "23399 1.81579 1.69035 0.0742135 0.931719 0.653358\n",
      "23499 1.61515 1.4612 0.10536 0.813459 0.657634\n",
      "23599 1.54198 1.42547 0.081732 0.841133 0.70429\n",
      "23699 1.81785 1.61609 0.124841 0.959855 0.606721\n",
      "23799 1.91328 1.70319 0.123351 1.10637 0.594202\n",
      "23899 1.49301 1.415 0.0551291 0.844948 0.67836\n",
      "23999 2.0712 1.92894 0.0737487 1.00956 0.580849\n",
      "24099 2.07077 1.89827 0.0908716 1.08058 0.60151\n",
      "24199 1.61993 1.52559 0.0618368 0.770106 0.612093\n",
      "24299 1.53528 1.44057 0.0657451 0.869262 0.647145\n",
      "24399 1.89746 1.79579 0.0566159 1.0707 0.690778\n",
      "24499 1.30473 1.20102 0.0863556 0.68576 0.743971\n",
      "24599 1.96951 1.85003 0.0645835 1.0113 0.569374\n",
      "24699 1.69623 1.61265 0.0518228 0.898525 0.653568\n",
      "24799 1.41081 1.31788 0.0705215 0.809346 0.676226\n",
      "24899 1.55893 1.47333 0.0581035 0.761155 0.769252\n",
      "24999 1.67629 1.56861 0.0686421 0.897374 0.629848\n",
      "25099 1.93441 1.77446 0.0901391 0.910889 0.62053\n",
      "25199 1.50209 1.38552 0.0841373 0.846167 0.657955\n",
      "25299 1.34647 1.2716 0.0588789 0.753834 0.736693\n",
      "25399 1.83606 1.70288 0.0782076 0.954846 0.600717\n",
      "25499 1.36341 1.26361 0.078985 0.788806 0.698961\n",
      "25599 1.94389 1.80122 0.0792034 0.980724 0.576582\n",
      "25699 1.50619 1.38457 0.0878441 0.885046 0.654963\n",
      "25799 1.57306 1.45863 0.0784544 0.912651 0.640966\n",
      "25899 1.67307 1.61381 0.0367188 0.890884 0.730777\n",
      "25999 1.65348 1.50623 0.0977601 0.865516 0.667235\n",
      "26099 1.35762 1.25413 0.0825216 0.775928 0.702647\n",
      "26199 1.53205 1.43416 0.0682544 0.773519 0.683471\n",
      "26299 1.70645 1.60432 0.0636582 1.01159 0.625877\n",
      "26399 1.79652 1.71556 0.0471914 1.01802 0.667021\n",
      "26499 1.71298 1.56746 0.0928382 0.830558 0.635013\n",
      "26599 1.93463 1.78162 0.0858836 0.925384 0.639034\n",
      "26699 1.49778 1.33852 0.118979 0.775528 0.667147\n",
      "26799 1.51832 1.42872 0.0627118 0.788532 0.700346\n",
      "26899 1.26016 1.18213 0.0660081 0.698631 0.722192\n",
      "26999 1.58264 1.44551 0.0948663 0.891752 0.662032\n",
      "27099 1.56544 1.457 0.0744248 0.889332 0.664295\n",
      "27199 1.5386 1.45823 0.0551089 0.863701 0.693649\n",
      "27299 1.53626 1.40972 0.0897658 0.836213 0.65971\n",
      "27399 1.51803 1.40745 0.078568 0.807635 0.694468\n",
      "27499 1.48443 1.343 0.105313 0.85331 0.675745\n",
      "27599 2.10352 1.96653 0.0696636 1.04052 0.635958\n",
      "27699 1.73029 1.60077 0.0809117 0.9219 0.615146\n",
      "27799 1.61943 1.5302 0.0583176 0.947043 0.651905\n",
      "27899 2.40213 2.16117 0.111496 1.16223 0.549002\n",
      "27999 1.67081 1.56022 0.0708813 0.902231 0.621897\n",
      "28099 1.33412 1.23167 0.0831771 0.56355 0.708826\n",
      "28199 1.83396 1.72342 0.0641397 0.916842 0.60072\n",
      "28299 1.29884 1.17686 0.103647 0.711807 0.72425\n",
      "28399 1.86289 1.74527 0.0673933 0.857768 0.665794\n",
      "28499 1.6599 1.52708 0.0869729 0.906939 0.679535\n",
      "28599 1.56576 1.4961 0.0465637 0.836867 0.698871\n",
      "28699 1.84936 1.6883 0.095402 0.957205 0.594546\n",
      "28799 1.48882 1.40865 0.0569106 0.747805 0.726328\n",
      "28899 1.89921 1.79409 0.0585895 1.06234 0.576527\n",
      "28999 1.80802 1.66442 0.0862783 0.910344 0.637346\n",
      "29099 1.78249 1.69446 0.0519523 0.9362 0.666294\n",
      "29199 1.43522 1.30009 0.103939 0.807793 0.687772\n",
      "29299 1.98365 1.83796 0.079266 0.922187 0.606218\n",
      "29399 1.67862 1.5553 0.0792862 0.830829 0.72693\n",
      "29499 1.63069 1.47481 0.105702 0.830792 0.659546\n",
      "29599 1.64379 1.53621 0.0700301 0.937724 0.607579\n",
      "29699 1.97895 1.86475 0.0612438 1.05076 0.623445\n",
      "29799 1.75501 1.61178 0.0888624 0.955689 0.630744\n",
      "29899 1.62617 1.46518 0.109878 0.869033 0.64111\n",
      "29999 1.40093 1.30696 0.0718969 0.797462 0.656111\n",
      "CPU times: user 2min 24s, sys: 9.53 s, total: 2min 33s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiEKQWAiEiYUgCIiISp/w8QKo2AHxeflAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f538c840390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#debug = False  # No debuging.\n",
    "\n",
    "x_dim = 1\n",
    "n_samples = 20\n",
    "y_dim = 1\n",
    "\n",
    "with tf.name_scope('X'):\n",
    "    x_within = tf.placeholder(shape=[n_samples, x_dim], dtype='float32',\n",
    "                              name='x_within')\n",
    "    x_boundary = tf.constant([[0.0], [1.0]], dtype='float32',\n",
    "                             name='x_boundary')\n",
    "    \n",
    "with tf.name_scope('HiddenLayers'):\n",
    "    hidden_layers = []\n",
    "    for i in range(3):\n",
    "        hidden_layers += [\n",
    "            lambda x: tf.layers.dense(\n",
    "                x, 10, activation=tf.nn.sigmoid),\n",
    "            lambda x: tf.layers.dropout(x),\n",
    "        ]\n",
    "ops = make_loss(\n",
    "    x_within, x_boundary,\n",
    "    lambda x: make_neural_network(x, hidden_layers, [y_dim]))\n",
    "\n",
    "# For optimizing\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train_op = optimizer.minimize(ops.loss)\n",
    "\n",
    "\n",
    "# For logging\n",
    "tf.summary.scalar('loss', ops.loss)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# -- Initializing\n",
    "# For logging\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.FileWriter('./logdir/'+ current_time, sess.graph)\n",
    "# Initialize all `tf.Variable`s, explicit or implicit\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# -- Optimizing\n",
    "\n",
    "# Parameters\n",
    "n_iters = 3*10**4\n",
    "\n",
    "def get_x_within_value():\n",
    "    x_within_value = np.array(\n",
    "        [[np.random.random()] for _ in range(n_samples)],\n",
    "        dtype='float32')\n",
    "    return x_within_value\n",
    "\n",
    "# Iterations\n",
    "for step in range(n_iters):\n",
    "    iter_ops = [train_op, summary_op, ops.loss, ops.y,\n",
    "                ops.grad_y, ops.action_part, ops.boundary_part]\n",
    "    feed_dict = {x_within: get_x_within_value()}\n",
    "    # No debugging\n",
    "    result = sess.run(iter_ops, feed_dict=feed_dict)\n",
    "    _, summary, loss, y, grad_y, action_part, boundary_part = result\n",
    "    '''\n",
    "    # Otherwise\n",
    "    if debug:\n",
    "        # Use `tdb`\n",
    "        if (step+1) % 10 == 0:\n",
    "            # Visualize\n",
    "            status, result = tdb.debug(\n",
    "                iter_ops, feed_dict=feed_dict, session=sess,\n",
    "                breakpoints=None, break_immediately=False)\n",
    "        else:\n",
    "            status, result = tdb.debug(\n",
    "                iter_ops, feed_dict=feed_dict, session=sess)\n",
    "            \n",
    "    else:\n",
    "        result = sess.run(iter_ops, feed_dict=feed_dict)\n",
    "    '''\n",
    "    if (step+1) % 100 == 0:\n",
    "        writer.add_summary(summary, step)\n",
    "        print(step, loss, action_part, boundary_part,\n",
    "              l1_norm(grad_y), l1_norm(y))\n",
    "\n",
    "# Return the predict-values of the trained neural network\n",
    "x_within_value = np.linspace(0, 1, n_samples, dtype='float32')\n",
    "x_within_value = np.expand_dims(x_within_value, axis=1)\n",
    "y_within_value = sess.run(\n",
    "    ops.y_within,\n",
    "    feed_dict={x_within: x_within_value})\n",
    "\n",
    "sess.close()\n",
    "\n",
    "plt.plot(x_within_value, y_within_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
